{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step:1 Extract the text PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "    :param pdf_path: Path to the PDF file.\n",
    "    :return: Extracted text as a string.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Extracts text from all PDF files in a directory and saves them to text files.\n",
    "    :param input_dir: Directory containing PDF files.\n",
    "    :param output_dir: Directory to save extracted text files.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pdf_file in input_path.glob(\"*.pdf\"):\n",
    "        print(f\"Processing {pdf_file.name}\")\n",
    "        text = extract_text_from_pdf(pdf_file)\n",
    "        output_file = output_path / f\"{pdf_file.stem}.txt\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        print(f\"Extracted text saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing kafka.pdf\n",
      "Extracted text saved to D:\\src_git\\LP\\LP\\projects\\paper_summarize\\summary\\kafka.txt\n",
      "Processing unikernels.pdf\n",
      "Extracted text saved to D:\\src_git\\LP\\LP\\projects\\paper_summarize\\summary\\unikernels.txt\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "extract_text_from_pdfs(\"D:\\\\src_git\\\\LP\\\\LP\\\\projects\\\\paper_summarize\\\\\", \"D:\\\\src_git\\\\LP\\\\LP\\\\projects\\\\paper_summarize\\\\summary\\\\\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(input_folder, output_file, task=\"summarization\"):\n",
    "    \"\"\"\n",
    "    Prepares a dataset for training a model.\n",
    "    :param input_folder: Folder containing text files extracted from PDFs.\n",
    "    :param output_file: Output JSON file for the dataset.\n",
    "    :param task: Task type - 'summarization' or 'qa' (question-answering).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(input_folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            if task == \"summarization\":\n",
    "                # Splitting into chunks (modify as per requirement)\n",
    "                chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n",
    "                for chunk in chunks:\n",
    "                    data.append({\n",
    "                        \"input_text\": chunk,\n",
    "                        \"summary\": \"Provide a concise summary of the above text.\"\n",
    "                    })\n",
    "            elif task == \"qa\":\n",
    "                # Example questions and answers for training (manual curation needed)\n",
    "                data.append({\n",
    "                    \"context\": text[:500],\n",
    "                    \"question\": \"What is the main topic of the document?\",\n",
    "                    \"answer\": \"Provide the primary topic.\"\n",
    "                })\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"Dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to summarization_dataset.json\n",
      "Dataset saved to qa_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "prepare_dataset(\"D:\\\\src_git\\\\LP\\\\LP\\\\projects\\\\paper_summarize\\\\summary\\\\\", \"summarization_dataset.json\", task=\"summarization\")\n",
    "prepare_dataset(\"D:\\\\src_git\\\\LP\\\\LP\\\\projects\\\\paper_summarize\\\\summary\\\\\", \"qa_dataset.json\", task=\"qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Train a hugging face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset_path, output_dir, model_name=\"t5-small\", task=\"summarization\"):\n",
    "    \"\"\"\n",
    "    Trains a Hugging Face model for summarization or question-answering.\n",
    "    :param dataset_path: Path to the training dataset (JSON format).\n",
    "    :param output_dir: Directory to save the trained model.\n",
    "    :param model_name: Base model to fine-tune.\n",
    "    :param task: Task type - 'summarization' or 'qa'.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('json', data_files={\"data\": dataset_path})[\"data\"]\n",
    "\n",
    "    # Split dataset into train and test\n",
    "    dataset_split = dataset.train_test_split(test_size=0.2, seed=42)  # 80% train, 20% test\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize data\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[\"input_text\"] if task == \"summarization\" else examples[\"context\"] + \" \" + examples[\"question\"]\n",
    "        targets = examples[\"summary\"] if task == \"summarization\" else examples[\"answer\"]\n",
    "        model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_data = dataset_split.map(preprocess_function, batched=True)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_data[\"train\"],\n",
    "        eval_dataset=tokenized_data[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating data split: 225 examples [00:00, 44998.97 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 4990.18 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:00<00:00, 2566.75 examples/s]\n",
      "d:\\src_git\\LP\\LP\\venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_28008\\1194072957.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  0%|          | 0/69 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "                                               \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 23/69 [00:13<00:22,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.8300540447235107, 'eval_runtime': 0.783, 'eval_samples_per_second': 57.474, 'eval_steps_per_second': 7.663, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 46/69 [00:25<00:10,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.967337131500244, 'eval_runtime': 0.784, 'eval_samples_per_second': 57.394, 'eval_steps_per_second': 7.653, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69/69 [00:38<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.699805498123169, 'eval_runtime': 0.7693, 'eval_samples_per_second': 58.497, 'eval_steps_per_second': 7.8, 'epoch': 3.0}\n",
      "{'train_runtime': 38.8011, 'train_samples_per_second': 13.917, 'train_steps_per_second': 1.778, 'train_loss': 3.7667722840240034, 'epoch': 3.0}\n",
      "Model saved to trained_model_summarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "train_model(\"summarization_dataset.json\", \"trained_model_summarization\", task=\"summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def load_model(model_dir):\n",
    "    \"\"\"\n",
    "    Loads a trained model and tokenizer from a directory.\n",
    "    :param model_dir: Path to the directory containing the saved model.\n",
    "    :return: Loaded tokenizer and model.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Example usage\n",
    "tokenizer, model = load_model(\"trained_model_summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_28008\\3477941638.py:15: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  input_text = \"\"\"Log  processing  has  become  a  critical  component  of  the  data \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: .  We have been using Kafka in production for some time and it is  processing hundreds of gigabytes of new data each day.\n"
     ]
    }
   ],
   "source": [
    "def test_model(tokenizer, model, input_text, max_length=128):\n",
    "    \"\"\"\n",
    "    Generates a prediction from the trained model.\n",
    "    :param tokenizer: Loaded tokenizer.\n",
    "    :param model: Loaded model.\n",
    "    :param input_text: Input text to summarize or query.\n",
    "    :param max_length: Maximum length of the generated output.\n",
    "    :return: Generated summary or answer.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"\"\"Log  processing  has  become  a  critical  component  of  the  data \\\n",
    "pipeline for consumer internet companies. We introduce Kafka, a \\\n",
    "distributed messaging system that we developed for collecting and \\\n",
    "delivering high volumes of log data with low latency. Our system \\\n",
    "incorporates  ideas  from  existing  log  aggregators  and  messaging \\ \n",
    "systems,  and  is  suitable  for  both  offline  and  online  message \\\n",
    "consumption.  We  made  quite  a  few  unconventional  yet  practical \\\n",
    "design choices in Kafka to make our system efficient and scalable. \\\n",
    "Our experimental results show that Kafka has superior \\\n",
    "performance  when  compared  to  two  popular  messaging  systems. \\ \n",
    "We  have  been  using  Kafka  in  production  for  some  time  and  it  is \\ \n",
    "processing hundreds of gigabytes of new data each day\"\"\"\n",
    "output = test_model(tokenizer, model, input_text)\n",
    "print(f\"Model Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to final_trained_model\n"
     ]
    }
   ],
   "source": [
    "def save_model(tokenizer, model, save_dir):\n",
    "    \"\"\"\n",
    "    Saves the trained model and tokenizer to a directory.\n",
    "    :param tokenizer: Tokenizer to save.\n",
    "    :param model: Model to save.\n",
    "    :param save_dir: Directory to save the model and tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    model.save_pretrained(save_dir)\n",
    "    print(f\"Model saved to {save_dir}\")\n",
    "\n",
    "# Example usage\n",
    "save_model(tokenizer, model, \"final_trained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Share and test the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_28008\\3425106651.py:5: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  input_text = \"\"\"Log  processing  has  become  a  critical  component  of  the  data \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded Model Output: .  We have been using Kafka in production for some time and it is  processing hundreds of gigabytes of new data each day.\n"
     ]
    }
   ],
   "source": [
    "# Reloading locally saved model\n",
    "reloaded_tokenizer, reloaded_model = load_model(\"final_trained_model\")\n",
    "\n",
    "# Testing the reloaded model\n",
    "input_text = \"\"\"Log  processing  has  become  a  critical  component  of  the  data \\\n",
    "pipeline for consumer internet companies. We introduce Kafka, a \\\n",
    "distributed messaging system that we developed for collecting and \\\n",
    "delivering high volumes of log data with low latency. Our system \\\n",
    "incorporates  ideas  from  existing  log  aggregators  and  messaging \\ \n",
    "systems,  and  is  suitable  for  both  offline  and  online  message \\\n",
    "consumption.  We  made  quite  a  few  unconventional  yet  practical \\\n",
    "design choices in Kafka to make our system efficient and scalable. \\\n",
    "Our experimental results show that Kafka has superior \\\n",
    "performance  when  compared  to  two  popular  messaging  systems. \\ \n",
    "We  have  been  using  Kafka  in  production  for  some  time  and  it  is \\ \n",
    "processing hundreds of gigabytes of new data each day\"\"\"\n",
    "output = test_model(reloaded_tokenizer, reloaded_model, input_text)\n",
    "print(f\"Reloaded Model Output: {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archiving the model for sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model archived as final_model_archive.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def archive_model(model_dir, archive_name):\n",
    "    \"\"\"\n",
    "    Archives a model directory into a .tar.gz file.\n",
    "    :param model_dir: Directory containing the model files.\n",
    "    :param archive_name: Name of the archive file (without extension).\n",
    "    \"\"\"\n",
    "    archive_path = f\"{archive_name}.tar.gz\"\n",
    "    shutil.make_archive(archive_name, 'gztar', model_dir)\n",
    "    print(f\"Model archived as {archive_path}\")\n",
    "\n",
    "# Example usage\n",
    "archive_model(\"final_trained_model\", \"final_model_archive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
