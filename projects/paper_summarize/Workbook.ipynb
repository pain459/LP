{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step:1 Extract the text PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "    :param pdf_path: Path to the PDF file.\n",
    "    :return: Extracted text as a string.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Extracts text from all PDF files in a directory and saves them to text files.\n",
    "    :param input_dir: Directory containing PDF files.\n",
    "    :param output_dir: Directory to save extracted text files.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pdf_file in input_path.glob(\"*.pdf\"):\n",
    "        print(f\"Processing {pdf_file.name}\")\n",
    "        text = extract_text_from_pdf(pdf_file)\n",
    "        output_file = output_path / f\"{pdf_file.stem}.txt\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        print(f\"Extracted text saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing kafka.pdf\n",
      "Extracted text saved to D:\\src_git\\LP\\LP\\projects\\paper_summarize\\summary\\kafka.txt\n",
      "Processing unikernels.pdf\n",
      "Extracted text saved to D:\\src_git\\LP\\LP\\projects\\paper_summarize\\summary\\unikernels.txt\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "extract_text_from_pdfs(\"D:\\\\src_git\\\\LP\\\\LP\\\\projects\\\\paper_summarize\\\\\", \"D:\\\\src_git\\\\LP\\\\LP\\\\projects\\\\paper_summarize\\\\summary\\\\\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(input_folder, output_file, task=\"summarization\"):\n",
    "    \"\"\"\n",
    "    Prepares a dataset for training a model.\n",
    "    :param input_folder: Folder containing text files extracted from PDFs.\n",
    "    :param output_file: Output JSON file for the dataset.\n",
    "    :param task: Task type - 'summarization' or 'qa' (question-answering).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(input_folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            if task == \"summarization\":\n",
    "                # Splitting into chunks (modify as per requirement)\n",
    "                chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n",
    "                for chunk in chunks:\n",
    "                    data.append({\n",
    "                        \"input_text\": chunk,\n",
    "                        \"summary\": \"Provide a concise summary of the above text.\"\n",
    "                    })\n",
    "            elif task == \"qa\":\n",
    "                # Example questions and answers for training (manual curation needed)\n",
    "                data.append({\n",
    "                    \"context\": text[:500],\n",
    "                    \"question\": \"What is the main topic of the document?\",\n",
    "                    \"answer\": \"Provide the primary topic.\"\n",
    "                })\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"Dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to summarization_dataset.json\n",
      "Dataset saved to qa_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "prepare_dataset(\"D:\\\\src_git\\\\LP\\\\LP\\\\projects\\\\paper_summarize\\\\summary\\\\\", \"summarization_dataset.json\", task=\"summarization\")\n",
    "prepare_dataset(\"D:\\\\src_git\\\\LP\\\\LP\\\\projects\\\\paper_summarize\\\\summary\\\\\", \"qa_dataset.json\", task=\"qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Train a hugging face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset_path, output_dir, model_name=\"t5-small\", task=\"summarization\"):\n",
    "    \"\"\"\n",
    "    Trains a Hugging Face model for summarization or question-answering.\n",
    "    :param dataset_path: Path to the training dataset (JSON format).\n",
    "    :param output_dir: Directory to save the trained model.\n",
    "    :param model_name: Base model to fine-tune.\n",
    "    :param task: Task type - 'summarization' or 'qa'.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('json', data_files={\"data\": dataset_path})[\"data\"]\n",
    "\n",
    "    # Split dataset into train and test\n",
    "    dataset_split = dataset.train_test_split(test_size=0.2, seed=42)  # 80% train, 20% test\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize data\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[\"input_text\"] if task == \"summarization\" else examples[\"context\"] + \" \" + examples[\"question\"]\n",
    "        targets = examples[\"summary\"] if task == \"summarization\" else examples[\"answer\"]\n",
    "        model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_data = dataset_split.map(preprocess_function, batched=True)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_data[\"train\"],\n",
    "        eval_dataset=tokenized_data[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating data split: 225 examples [00:00, 44998.97 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 4990.18 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:00<00:00, 2566.75 examples/s]\n",
      "d:\\src_git\\LP\\LP\\venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_28008\\1194072957.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  0%|          | 0/69 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "                                               \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 23/69 [00:13<00:22,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.8300540447235107, 'eval_runtime': 0.783, 'eval_samples_per_second': 57.474, 'eval_steps_per_second': 7.663, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 46/69 [00:25<00:10,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.967337131500244, 'eval_runtime': 0.784, 'eval_samples_per_second': 57.394, 'eval_steps_per_second': 7.653, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69/69 [00:38<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.699805498123169, 'eval_runtime': 0.7693, 'eval_samples_per_second': 58.497, 'eval_steps_per_second': 7.8, 'epoch': 3.0}\n",
      "{'train_runtime': 38.8011, 'train_samples_per_second': 13.917, 'train_steps_per_second': 1.778, 'train_loss': 3.7667722840240034, 'epoch': 3.0}\n",
      "Model saved to trained_model_summarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "train_model(\"summarization_dataset.json\", \"trained_model_summarization\", task=\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
