{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: torch in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (2.5.1)\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: filelock in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (5.28.3)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.3)\n",
      "Requirement already satisfied: namex in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\src_git\\lp\\lp\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.6/1.7 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 5.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Loading the Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\src_git\\LP\\LP\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the summarization model\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Preprocessing the Subtitle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_subtitles(file_path):\n",
    "    \"\"\"\n",
    "    Clean up and prepare subtitle text by removing timestamps, line numbers, and non-dialogue cues.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the subtitle (.srt) file.\n",
    "\n",
    "    Returns:\n",
    "    - str: Cleaned subtitle text.\n",
    "    \"\"\"\n",
    "    # Read the subtitle file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Remove BOM (Byte Order Mark) if present at the start of the file\n",
    "    content = content.lstrip(\"\\ufeff\")\n",
    "    \n",
    "    # Remove timestamps (format: 00:00:35,202 --> 00:00:37,538)\n",
    "    content = re.sub(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', '', content)\n",
    "    \n",
    "    # Remove line numbers (appear as standalone numbers in the text)\n",
    "    content = re.sub(r'\\n\\d+\\n', '\\n', content)\n",
    "    \n",
    "    # Remove non-dialogue cues such as (MUSIC PLAYING) or <i>italic text</i>\n",
    "    content = re.sub(r'\\(.*?\\)', '', content)  # Remove text within parentheses\n",
    "    content = re.sub(r'<.*?>', '', content)    # Remove HTML-like tags within <>\n",
    "\n",
    "    # Replace multiple newlines with a single space for better readability\n",
    "    content = re.sub(r'\\n+', ' ', content)\n",
    "    content = content.strip()  # Remove any leading or trailing whitespace\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Chunk the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_length=500):\n",
    "    \"\"\"\n",
    "    Split cleaned text into manageable chunks for summarization.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The cleaned subtitle text.\n",
    "    - max_length (int): Maximum number of words per chunk.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of text chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    # Create chunks of specified max length\n",
    "    for i in range(0, len(words), max_length):\n",
    "        chunk = \" \".join(words[i:i + max_length])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_summarizer():\n",
    "    \"\"\"\n",
    "    Load the Hugging Face summarization pipeline using the BART model.\n",
    "\n",
    "    Returns:\n",
    "    - Pipeline: Hugging Face summarization pipeline.\n",
    "    \"\"\"\n",
    "    return pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Summarize Each Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(chunks, summarizer):\n",
    "    \"\"\"\n",
    "    Summarize each chunk individually and combine results.\n",
    "    \n",
    "    Parameters:\n",
    "    - chunks (list): List of text chunks.\n",
    "    - summarizer (Pipeline): Loaded summarization pipeline.\n",
    "\n",
    "    Returns:\n",
    "    - str: Combined summary of all chunks.\n",
    "    \"\"\"\n",
    "    chunk_summaries = []\n",
    "    for chunk in chunks:\n",
    "        # Summarize each chunk and append to results list\n",
    "        summary = summarizer(chunk, max_length=100, min_length=30, do_sample=False)\n",
    "        chunk_summaries.append(summary[0]['summary_text'])\n",
    "    \n",
    "    # Combine all chunk summaries into one large text\n",
    "    return \" \".join(chunk_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_summary(text, summarizer, section_length=400, max_input_length=1024):\n",
    "    \"\"\"\n",
    "    Create a final summary by breaking down the combined summary into smaller sections if necessary.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): Combined summary text from all chunks.\n",
    "    - summarizer (Pipeline): Loaded summarization pipeline.\n",
    "    - section_length (int): Word length of each section for further summarization.\n",
    "    - max_input_length (int): Maximum length of input text tokens for the final summary.\n",
    "\n",
    "    Returns:\n",
    "    - str: Final summarized text.\n",
    "    \"\"\"\n",
    "    # Split combined text into smaller sections\n",
    "    words = text.split()\n",
    "    sections = []\n",
    "    for i in range(0, len(words), section_length):\n",
    "        section = \" \".join(words[i:i + section_length])\n",
    "        sections.append(section)\n",
    "    \n",
    "    # Summarize each section separately\n",
    "    section_summaries = []\n",
    "    for section in sections:\n",
    "        summary = summarizer(section, max_length=120, min_length=40, do_sample=False)\n",
    "        section_summaries.append(summary[0]['summary_text'])\n",
    "    \n",
    "    # Combine section summaries into a single text\n",
    "    combined_summary = \" \".join(section_summaries)\n",
    "    \n",
    "    # If still large, truncate to max_input_length for one final summarization pass\n",
    "    if len(combined_summary.split()) > max_input_length:\n",
    "        combined_summary = \" \".join(combined_summary.split()[:max_input_length])\n",
    "    \n",
    "    # Summarize the combined text to get the final movie summary\n",
    "    final_summary = summarizer(combined_summary, max_length=150, min_length=50, do_sample=False)\n",
    "    return final_summary[0]['summary_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function to Run the Summarization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_movie_summary(file_path):\n",
    "    \"\"\"\n",
    "    Generate a movie summary from subtitles by processing and summarizing in stages.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the subtitle file.\n",
    "\n",
    "    Returns:\n",
    "    - str: Final summarized text representing the movie plot.\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess the subtitle file to clean the text\n",
    "    cleaned_text = preprocess_subtitles(file_path)\n",
    "    \n",
    "    # Step 2: Split cleaned text into manageable chunks\n",
    "    chunks = chunk_text(cleaned_text, max_length=500)\n",
    "    \n",
    "    # Step 3: Load the summarization model\n",
    "    summarizer = load_summarizer()\n",
    "    \n",
    "    # Step 4: Summarize each chunk and combine the summaries\n",
    "    combined_summary = summarize_chunks(chunks, summarizer)\n",
    "    print(\"Combined summary length (in words):\", len(combined_summary.split()))  # Debugging line\n",
    "    \n",
    "    # Step 5: Generate a final summary from the combined chunk summaries\n",
    "    movie_summary = final_summary(combined_summary, summarizer)\n",
    "    return movie_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File path to the subtitle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:\\\\src_git\\\\LP\\\\LP\\\\projects\\\\video_subtitle_summary\\\\subtitle_summary\\\\avatar_twow.srt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the movie summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined summary length (in words): 856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 120, but your input_length is only 92. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n",
      "Your max_length is set to 150, but your input_length is only 149. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=74)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Summary:\n",
      " Jake Sully and his squad are on a mission to hunt down and kill the leader of the Na'vi insurgency. The song is sung by the Sea People, who live on the island of Neteyam. It is set to music by ZOE SALDAÑA.\n"
     ]
    }
   ],
   "source": [
    "summary = generate_movie_summary(file_path)\n",
    "print(\"Movie Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
