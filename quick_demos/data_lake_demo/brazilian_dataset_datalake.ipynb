{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Data Lake for E-commerce Data Analytics Using Real-Life Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (3.5.2)\n",
      "Requirement already satisfied: pandas in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (2.2.2)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (from pandas) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Collecting bleach\n",
      "  Downloading bleach-6.1.0-py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.8/162.8 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2023.7.22 in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (from kaggle) (2024.7.4)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: requests in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: six>=1.10 in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (from kaggle) (1.16.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (from kaggle) (2.2.2)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (from requests->kaggle) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ravik/src_git/LP/venv/lib/python3.10/site-packages (from requests->kaggle) (3.3.2)\n",
      "Using legacy 'setup.py install' for kaggle, since package 'wheel' is not installed.\n",
      "Installing collected packages: webencodings, text-unidecode, tqdm, python-slugify, bleach, kaggle\n",
      "  Running setup.py install for kaggle ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed bleach-6.1.0 kaggle-1.6.17 python-slugify-8.0.4 text-unidecode-1.3 tqdm-4.66.5 webencodings-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark pandas kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ravik/.kaggle/kaggle.json'\n",
      "Dataset URL: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
      "License(s): CC-BY-NC-SA-4.0\n",
      "Downloading brazilian-ecommerce.zip to ./brazil_data_lake/raw\n",
      "100%|██████████████████████████████████████| 42.6M/42.6M [00:04<00:00, 9.97MB/s]\n",
      "100%|██████████████████████████████████████| 42.6M/42.6M [00:04<00:00, 9.81MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d olistbr/brazilian-ecommerce -p ./brazil_data_lake/raw --unzip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest the data into the datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define directories\n",
    "base_dir = Path('./brazil_data_lake')\n",
    "raw_dir = base_dir / 'raw'\n",
    "processed_dir = base_dir / 'processed'\n",
    "cleaned_dir = base_dir / 'cleaned'\n",
    "\n",
    "# Create directories\n",
    "for dir in [raw_dir, processed_dir, cleaned_dir]:\n",
    "    dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Data (ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/08/25 08:32:08 WARN Utils: Your hostname, DOOM resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/08/25 08:32:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/25 08:32:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/25 08:32:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/08/25 08:32:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/08/25 08:32:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/08/25 08:32:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/08/25 08:32:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/08/25 08:32:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/08/25 08:32:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "24/08/25 08:32:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "24/08/25 08:32:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "24/08/25 08:32:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 47.50% for 16 writers\n",
      "24/08/25 08:32:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "24/08/25 08:32:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "24/08/25 08:32:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "24/08/25 08:32:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/08/25 08:32:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/08/25 08:32:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/08/25 08:32:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/08/25 08:32:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('EcommerceDataLake').getOrCreate()\n",
    "\n",
    "# Load data into Spark DataFrames\n",
    "orders_df = spark.read.csv(str(raw_dir / 'olist_orders_dataset.csv'), header=True, inferSchema=True)\n",
    "customers_df = spark.read.csv(str(raw_dir / 'olist_customers_dataset.csv'), header=True, inferSchema=True)\n",
    "order_items_df = spark.read.csv(str(raw_dir / 'olist_order_items_dataset.csv'), header=True, inferSchema=True)\n",
    "products_df = spark.read.csv(str(raw_dir / 'olist_products_dataset.csv'), header=True, inferSchema=True)\n",
    "\n",
    "# Example Transformation: Join orders with customer data and order items\n",
    "enriched_orders_df = orders_df \\\n",
    "    .join(customers_df, on=\"customer_id\", how=\"left\") \\\n",
    "    .join(order_items_df, on=\"order_id\", how=\"left\") \\\n",
    "    .join(products_df, on=\"product_id\", how=\"left\")\n",
    "\n",
    "# Save processed data\n",
    "enriched_orders_df.write.parquet(str(processed_dir / 'orders_enriched.parquet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
